{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"torchplate : Minimal Experiment Workflows in PyTorch ( Github | PyPI | Documentation ) Installation | Example | More examples | Starter project An extremely minimal and simple experiment module for machine learning in PyTorch (PyTorch + boilerplate = torchplate ). In addition to abstracting away the training loop, we provide several abstractions to improve the efficiency of machine learning workflows with PyTorch. Installation $ pip install torchplate Example To get started, create a child class of torchplate.experiment.Experiment and provide several key, experiment-unique items: model, optimizer, and a training set dataloader. Then, provide an implementation of the abstract method evaluate . This function takes in a batch from the trainloader and should return the loss (i.e., implement the forward pass + loss calculation). Add whatever custom methods you may want to this class. Then starting training! That's it! import torchplate from torchplate import experiment from torchplate import utils import torch import torch.optim as optim import torch.nn as nn import torch.nn.functional as F import requests import cloudpickle as cp from urllib.request import urlopen class Net(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(3*32*32, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 3) def forward(self, x): x = torch.flatten(x, 1) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x class CifarExp(torchplate.experiment.Experiment): def __init__(self): self.model = Net() self.optimizer = optim.Adam(self.model.parameters(), lr=0.001) self.criterion = nn.CrossEntropyLoss() dataset = cp.load(urlopen(\"https://stanford.edu/~rsikand/assets/datasets/mini_cifar.pkl\")) # use various torchplate.utils to improve efficiency of common workflows self.trainloader, self.testloader = torchplate.utils.get_xy_loaders(dataset) # inherit from torchplate.experiment.Experiment and pass in # model, optimizer, and dataloader super().__init__( model = self.model, optimizer = self.optimizer, trainloader = self.trainloader, verbose = True ) # provide this abstract method to calculate loss def evaluate(self, batch): x, y = batch logits = self.model(x) loss_val = self.criterion(logits, y) return loss_val def test(self): accuracy_count = 0 for x, y in self.testloader: logits = self.model(x) pred = torch.argmax(F.softmax(logits, dim=1)).item() print(f\"Prediction: {pred}, True: {y.item()}\") if pred == y: accuracy_count += 1 print(\"Accuracy: \", accuracy_count/len(self.testloader)) def on_epoch_end(self): # to illustrate the concept of callbacks print(\"------------------ (Epoch end) --------------------\") exp = CifarExp() exp.train(num_epochs=100) exp.test() More examples See examples/starter for a full program example. To get started running your own experiments, you can use examples/starter as a base (or use cookiecutter as shown below). Starter project The starter branch holds the source for a cookiecutter project. This allows users to easily create projects from the starter code example by running a simple command. To get started, install cookiecutter and then type $ cookiecutter https://github.com/rosikand/torchplate.git --checkout starter which will generate the following structure for you to use as a base for your projects: torchplate_starter \u251c\u2500\u2500 datasets.py \u251c\u2500\u2500 experiments.py \u251c\u2500\u2500 models.py \u2514\u2500\u2500 runner.py","title":"Home"},{"location":"#torchplate-minimal-experiment-workflows-in-pytorch","text":"( Github | PyPI | Documentation ) Installation | Example | More examples | Starter project An extremely minimal and simple experiment module for machine learning in PyTorch (PyTorch + boilerplate = torchplate ). In addition to abstracting away the training loop, we provide several abstractions to improve the efficiency of machine learning workflows with PyTorch.","title":"torchplate: Minimal Experiment Workflows in PyTorch"},{"location":"#installation","text":"$ pip install torchplate","title":"Installation"},{"location":"#example","text":"To get started, create a child class of torchplate.experiment.Experiment and provide several key, experiment-unique items: model, optimizer, and a training set dataloader. Then, provide an implementation of the abstract method evaluate . This function takes in a batch from the trainloader and should return the loss (i.e., implement the forward pass + loss calculation). Add whatever custom methods you may want to this class. Then starting training! That's it! import torchplate from torchplate import experiment from torchplate import utils import torch import torch.optim as optim import torch.nn as nn import torch.nn.functional as F import requests import cloudpickle as cp from urllib.request import urlopen class Net(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(3*32*32, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 3) def forward(self, x): x = torch.flatten(x, 1) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x class CifarExp(torchplate.experiment.Experiment): def __init__(self): self.model = Net() self.optimizer = optim.Adam(self.model.parameters(), lr=0.001) self.criterion = nn.CrossEntropyLoss() dataset = cp.load(urlopen(\"https://stanford.edu/~rsikand/assets/datasets/mini_cifar.pkl\")) # use various torchplate.utils to improve efficiency of common workflows self.trainloader, self.testloader = torchplate.utils.get_xy_loaders(dataset) # inherit from torchplate.experiment.Experiment and pass in # model, optimizer, and dataloader super().__init__( model = self.model, optimizer = self.optimizer, trainloader = self.trainloader, verbose = True ) # provide this abstract method to calculate loss def evaluate(self, batch): x, y = batch logits = self.model(x) loss_val = self.criterion(logits, y) return loss_val def test(self): accuracy_count = 0 for x, y in self.testloader: logits = self.model(x) pred = torch.argmax(F.softmax(logits, dim=1)).item() print(f\"Prediction: {pred}, True: {y.item()}\") if pred == y: accuracy_count += 1 print(\"Accuracy: \", accuracy_count/len(self.testloader)) def on_epoch_end(self): # to illustrate the concept of callbacks print(\"------------------ (Epoch end) --------------------\") exp = CifarExp() exp.train(num_epochs=100) exp.test()","title":"Example"},{"location":"#more-examples","text":"See examples/starter for a full program example. To get started running your own experiments, you can use examples/starter as a base (or use cookiecutter as shown below).","title":"More examples"},{"location":"#starter-project","text":"The starter branch holds the source for a cookiecutter project. This allows users to easily create projects from the starter code example by running a simple command. To get started, install cookiecutter and then type $ cookiecutter https://github.com/rosikand/torchplate.git --checkout starter which will generate the following structure for you to use as a base for your projects: torchplate_starter \u251c\u2500\u2500 datasets.py \u251c\u2500\u2500 experiments.py \u251c\u2500\u2500 models.py \u2514\u2500\u2500 runner.py","title":"Starter project"},{"location":"changelog/","text":"Changelog 0.0.8 Fixed metrics import bug. 0.0.7 Largest change to date. New features: gradient accumulation, save weights every $n$ epochs, display batch loss, metrics, metrics interfaced with train . 0.0.6 Fixed bug in model weight saving. 0.0.5 Added model weights loading and saving. 0.0.4 Several changes: added callbacks, changed verbose default to true, added ModelInterface pipeline to utils . 0.0.3 Added verbose option as well as wandb logging 0.0.2 Fixed a polymorphic bug 0.0.1 First version published. Provides basic data-loading utilities and the base experiment module.","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#008","text":"Fixed metrics import bug.","title":"0.0.8"},{"location":"changelog/#007","text":"Largest change to date. New features: gradient accumulation, save weights every $n$ epochs, display batch loss, metrics, metrics interfaced with train .","title":"0.0.7"},{"location":"changelog/#006","text":"Fixed bug in model weight saving.","title":"0.0.6"},{"location":"changelog/#005","text":"Added model weights loading and saving.","title":"0.0.5"},{"location":"changelog/#004","text":"Several changes: added callbacks, changed verbose default to true, added ModelInterface pipeline to utils .","title":"0.0.4"},{"location":"changelog/#003","text":"Added verbose option as well as wandb logging","title":"0.0.3"},{"location":"changelog/#002","text":"Fixed a polymorphic bug","title":"0.0.2"},{"location":"changelog/#001","text":"First version published. Provides basic data-loading utilities and the base experiment module.","title":"0.0.1"},{"location":"experiment/","text":"Experiment module This page provides an overview of the main module in torchplate: torchplate.experiment which houses torchplate.experiment.Experiment . (to be updated...)","title":"Experiment module"},{"location":"experiment/#experiment-module","text":"This page provides an overview of the main module in torchplate: torchplate.experiment which houses torchplate.experiment.Experiment . (to be updated...)","title":"Experiment module"},{"location":"metrics/","text":"Metrics module to be updated...","title":"Metrics module"},{"location":"metrics/#metrics-module","text":"to be updated...","title":"Metrics module"},{"location":"utils/","text":"Utils module This page provides an overview of the torchplate.utils which provides utility functions useful for common ML workflows. (to be updated...)","title":"Utils module"},{"location":"utils/#utils-module","text":"This page provides an overview of the torchplate.utils which provides utility functions useful for common ML workflows. (to be updated...)","title":"Utils module"},{"location":"api/experiment/","text":"File: experiment.py Provides the main module of the package: Experiment. Experiment Bases: ABC Base experiment superclass. All other experiments should inherit from this class. Each sub-experiment must provide an implementation of the \"evaluate\" abstract method. A sub-experiment has full autonomy to override the basic components such as the training loop \"train\". Source code in torchplate/experiment.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 class Experiment ( ABC ): \"\"\" Base experiment superclass. All other experiments should inherit from this class. Each sub-experiment must provide an implementation of the \"evaluate\" abstract method. A sub-experiment has full autonomy to override the basic components such as the training loop \"train\". \"\"\" def __init__ ( self , model , optimizer , trainloader , save_weights_every_n_epochs = None , wandb_logger = None , verbose = True , experiment_name = misc . timestamp ()): \"\"\" Experiment superclass initializer. Each subclass must provide a model, optimizer, and trainloader at the very least. Arguments: ----------- - model: torch nn.module - optimizer: torch optimizer - trainloader: torch Dataloader to be used for training Optional: - save_weights_every_n_epochs: how often to save the model weight automatically. Default: None. Specify None if you don't want to save weights automatically. - wandb_logger (wandb.init object): pass in if you want to log to wandb. Default: None. - verbose (boolean): if true, print out metrics during training. Default: True. - experiment_name (str): name of the experiment for saving. Default: timestamp. \"\"\" self . model = model self . optimizer = optimizer self . trainloader = trainloader self . wandb_logger = wandb_logger self . verbose = verbose self . save_weights_every_n_epochs = save_weights_every_n_epochs assert type ( self . save_weights_every_n_epochs ) is int or self . save_weights_every_n_epochs is None , \"save_weights_every_n_epochs must be an integer or None\" self . epoch_num = 0 self . experiment_name = experiment_name assert type ( self . experiment_name ) is str , \"experiment name must be a string\" def train ( self , num_epochs , gradient_accumulate_every_n_batches = 1 , display_batch_loss = False ): \"\"\" Training loop. Can optionally specify how often to accumulate gradients. Default: 1. \"\"\" self . on_run_start () self . model . train () metrics_ = {} first_batch = True eval_returns_loss_only = False history = {} for epoch in range ( num_epochs ): self . epoch_num += 1 self . on_epoch_start () running_loss = 0.0 tqdm_loader = tqdm ( self . trainloader ) batch_idx = - 1 for batch in tqdm_loader : batch_idx += 1 if not display_batch_loss : tqdm_loader . set_description ( f \"Epoch { self . epoch_num } \" ) self . on_batch_start () evals = self . evaluate ( batch ) # registration of metrics if first_batch : if torch . is_tensor ( evals ): if evals . dim () == 0 : eval_returns_loss_only = True history [ \"loss\" ] = [] else : assert type ( evals ) is dict , \"if you aren't providing a scalar loss value in evaluate, you must return a dictionary.\" assert \"loss\" in evals . keys (), \"evaluate must return a 'loss' value\" # register metrics for key in evals : curr_metric = metrics . MeanMetric () metrics_ [ key ] = curr_metric history [ key ] = [] first_batch = False # get loss val if eval_returns_loss_only : loss = evals else : loss = evals [ \"loss\" ] if display_batch_loss : tqdm_loader . set_description ( f \"Epoch { self . epoch_num } | loss: { loss : .4f } \" ) loss . backward () # gradient accumulation if gradient_accumulate_every_n_batches > 1 : loss = loss / gradient_accumulate_every_n_batches if (( batch_idx + 1 ) % gradient_accumulate_every_n_batches == 0 ) or ( batch_idx + 1 == len ( self . trainloader )): self . optimizer . step () self . optimizer . zero_grad () else : self . optimizer . step () self . optimizer . zero_grad () # per-batch updates if not eval_returns_loss_only : # update metrics for key in metrics_ : metrics_ [ key ] . update ( evals [ key ]) running_loss += loss . item () self . on_batch_end () # per-epoch updates epoch_avg_loss = running_loss / len ( self . trainloader ) if self . wandb_logger is not None : if eval_returns_loss_only : self . wandb_logger . log ({ \"Training loss\" : epoch_avg_loss }) else : self . wandb_logger . log ( metrics_ ) if self . verbose : if eval_returns_loss_only : print ( \"Training Loss (epoch \" + str ( self . epoch_num ) + \"):\" , epoch_avg_loss ) else : for key in metrics_ : print ( f \"Training { key } (epoch { str ( self . epoch_num ) } ): { metrics_ [ key ] . get () } \" ) # reset metrics and update history if not eval_returns_loss_only : for key in metrics_ : append_val = metrics_ [ key ] . get () if torch . is_tensor ( append_val ): if append_val . requires_grad : append_val = append_val . detach () . cpu () . item () history [ key ] . append ( append_val ) metrics_ [ key ] . reset () else : history [ \"loss\" ] . append ( epoch_avg_loss ) # weight saving if self . save_weights_every_n_epochs is not None : if self . epoch_num % self . save_weights_every_n_epochs == 0 : if not os . path . exists ( \"saved\" ): os . makedirs ( \"saved\" ) save_path = \"saved/epoch_\" + str ( self . epoch_num ) + \"-\" + self . experiment_name self . save_weights ( save_path ) self . on_epoch_end () self . model . eval () print ( 'Finished Training!' ) self . epoch_num = 0 self . on_run_end () return history @abstractmethod def evaluate ( self , batch ): \"\"\" Abstract method which the user must provide. Implement the forward pass and return the loss value. Arguments: ----------- - batch: a batch from the train data loader (i.e., an (x, y) pair). To be used as input into the model. Returns: ----------- - A scalar loss value. \"\"\" pass def on_batch_start ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen before each batch iteration. \"\"\" pass def on_batch_end ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen after each batch iteration. \"\"\" pass def on_epoch_start ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen before each epoch iteration. \"\"\" pass def on_epoch_end ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen after each epoch iteration. \"\"\" pass def on_run_start ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen before each run. \"\"\" pass def on_run_end ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen after each run. \"\"\" pass def save_weights ( self , save_path = None ): \"\"\" Function to save model weights at 'save_path'. Arguments: - save_path: path to save the weights. If not given, defaults to current timestamp. \"\"\" if save_path is None : if not os . path . exists ( \"saved\" ): os . makedirs ( \"saved\" ) save_path = \"saved/\" + misc . timestamp () + \".pth\" torch . save ( self . model . state_dict (), save_path ) print ( \"Model weights saved at: \" + str ( save_path )) def load_weights ( self , weight_path ): \"\"\" Function to load model weights saved at 'weight_path'. Arguments: - weight_path: path pointing to the saved weights. \"\"\" self . model . load_state_dict ( torch . load ( weight_path )) print ( \"Weights loaded!\" ) __init__ ( model , optimizer , trainloader , save_weights_every_n_epochs = None , wandb_logger = None , verbose = True , experiment_name = misc . timestamp ()) Experiment superclass initializer. Each subclass must provide a model, optimizer, and trainloader at the very least. Arguments: model: torch nn.module optimizer: torch optimizer trainloader: torch Dataloader to be used for training Optional: save_weights_every_n_epochs: how often to save the model weight automatically. Default: None. Specify None if you don't want to save weights automatically. wandb_logger (wandb.init object): pass in if you want to log to wandb. Default: None. verbose (boolean): if true, print out metrics during training. Default: True. experiment_name (str): name of the experiment for saving. Default: timestamp. Source code in torchplate/experiment.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , model , optimizer , trainloader , save_weights_every_n_epochs = None , wandb_logger = None , verbose = True , experiment_name = misc . timestamp ()): \"\"\" Experiment superclass initializer. Each subclass must provide a model, optimizer, and trainloader at the very least. Arguments: ----------- - model: torch nn.module - optimizer: torch optimizer - trainloader: torch Dataloader to be used for training Optional: - save_weights_every_n_epochs: how often to save the model weight automatically. Default: None. Specify None if you don't want to save weights automatically. - wandb_logger (wandb.init object): pass in if you want to log to wandb. Default: None. - verbose (boolean): if true, print out metrics during training. Default: True. - experiment_name (str): name of the experiment for saving. Default: timestamp. \"\"\" self . model = model self . optimizer = optimizer self . trainloader = trainloader self . wandb_logger = wandb_logger self . verbose = verbose self . save_weights_every_n_epochs = save_weights_every_n_epochs assert type ( self . save_weights_every_n_epochs ) is int or self . save_weights_every_n_epochs is None , \"save_weights_every_n_epochs must be an integer or None\" self . epoch_num = 0 self . experiment_name = experiment_name assert type ( self . experiment_name ) is str , \"experiment name must be a string\" evaluate ( batch ) abstractmethod Abstract method which the user must provide. Implement the forward pass and return the loss value. batch: a batch from the train data loader (i.e., an (x, y) pair). To be used as input into the model. A scalar loss value. Source code in torchplate/experiment.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 @abstractmethod def evaluate ( self , batch ): \"\"\" Abstract method which the user must provide. Implement the forward pass and return the loss value. Arguments: ----------- - batch: a batch from the train data loader (i.e., an (x, y) pair). To be used as input into the model. Returns: ----------- - A scalar loss value. \"\"\" pass load_weights ( weight_path ) Function to load model weights saved at 'weight_path'. weight_path: path pointing to the saved weights. Source code in torchplate/experiment.py 255 256 257 258 259 260 261 262 def load_weights ( self , weight_path ): \"\"\" Function to load model weights saved at 'weight_path'. Arguments: - weight_path: path pointing to the saved weights. \"\"\" self . model . load_state_dict ( torch . load ( weight_path )) print ( \"Weights loaded!\" ) on_batch_end () Callback that can be overriden. Implement whatever you want to happen after each batch iteration. Source code in torchplate/experiment.py 201 202 203 204 205 206 def on_batch_end ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen after each batch iteration. \"\"\" pass on_batch_start () Callback that can be overriden. Implement whatever you want to happen before each batch iteration. Source code in torchplate/experiment.py 193 194 195 196 197 198 def on_batch_start ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen before each batch iteration. \"\"\" pass on_epoch_end () Callback that can be overriden. Implement whatever you want to happen after each epoch iteration. Source code in torchplate/experiment.py 217 218 219 220 221 222 def on_epoch_end ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen after each epoch iteration. \"\"\" pass on_epoch_start () Callback that can be overriden. Implement whatever you want to happen before each epoch iteration. Source code in torchplate/experiment.py 209 210 211 212 213 214 def on_epoch_start ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen before each epoch iteration. \"\"\" pass on_run_end () Callback that can be overriden. Implement whatever you want to happen after each run. Source code in torchplate/experiment.py 233 234 235 236 237 238 def on_run_end ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen after each run. \"\"\" pass on_run_start () Callback that can be overriden. Implement whatever you want to happen before each run. Source code in torchplate/experiment.py 225 226 227 228 229 230 def on_run_start ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen before each run. \"\"\" pass save_weights ( save_path = None ) Function to save model weights at 'save_path'. save_path: path to save the weights. If not given, defaults to current timestamp. Source code in torchplate/experiment.py 241 242 243 244 245 246 247 248 249 250 251 252 def save_weights ( self , save_path = None ): \"\"\" Function to save model weights at 'save_path'. Arguments: - save_path: path to save the weights. If not given, defaults to current timestamp. \"\"\" if save_path is None : if not os . path . exists ( \"saved\" ): os . makedirs ( \"saved\" ) save_path = \"saved/\" + misc . timestamp () + \".pth\" torch . save ( self . model . state_dict (), save_path ) print ( \"Model weights saved at: \" + str ( save_path )) train ( num_epochs , gradient_accumulate_every_n_batches = 1 , display_batch_loss = False ) Training loop. Can optionally specify how often to accumulate gradients. Default: 1. Source code in torchplate/experiment.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def train ( self , num_epochs , gradient_accumulate_every_n_batches = 1 , display_batch_loss = False ): \"\"\" Training loop. Can optionally specify how often to accumulate gradients. Default: 1. \"\"\" self . on_run_start () self . model . train () metrics_ = {} first_batch = True eval_returns_loss_only = False history = {} for epoch in range ( num_epochs ): self . epoch_num += 1 self . on_epoch_start () running_loss = 0.0 tqdm_loader = tqdm ( self . trainloader ) batch_idx = - 1 for batch in tqdm_loader : batch_idx += 1 if not display_batch_loss : tqdm_loader . set_description ( f \"Epoch { self . epoch_num } \" ) self . on_batch_start () evals = self . evaluate ( batch ) # registration of metrics if first_batch : if torch . is_tensor ( evals ): if evals . dim () == 0 : eval_returns_loss_only = True history [ \"loss\" ] = [] else : assert type ( evals ) is dict , \"if you aren't providing a scalar loss value in evaluate, you must return a dictionary.\" assert \"loss\" in evals . keys (), \"evaluate must return a 'loss' value\" # register metrics for key in evals : curr_metric = metrics . MeanMetric () metrics_ [ key ] = curr_metric history [ key ] = [] first_batch = False # get loss val if eval_returns_loss_only : loss = evals else : loss = evals [ \"loss\" ] if display_batch_loss : tqdm_loader . set_description ( f \"Epoch { self . epoch_num } | loss: { loss : .4f } \" ) loss . backward () # gradient accumulation if gradient_accumulate_every_n_batches > 1 : loss = loss / gradient_accumulate_every_n_batches if (( batch_idx + 1 ) % gradient_accumulate_every_n_batches == 0 ) or ( batch_idx + 1 == len ( self . trainloader )): self . optimizer . step () self . optimizer . zero_grad () else : self . optimizer . step () self . optimizer . zero_grad () # per-batch updates if not eval_returns_loss_only : # update metrics for key in metrics_ : metrics_ [ key ] . update ( evals [ key ]) running_loss += loss . item () self . on_batch_end () # per-epoch updates epoch_avg_loss = running_loss / len ( self . trainloader ) if self . wandb_logger is not None : if eval_returns_loss_only : self . wandb_logger . log ({ \"Training loss\" : epoch_avg_loss }) else : self . wandb_logger . log ( metrics_ ) if self . verbose : if eval_returns_loss_only : print ( \"Training Loss (epoch \" + str ( self . epoch_num ) + \"):\" , epoch_avg_loss ) else : for key in metrics_ : print ( f \"Training { key } (epoch { str ( self . epoch_num ) } ): { metrics_ [ key ] . get () } \" ) # reset metrics and update history if not eval_returns_loss_only : for key in metrics_ : append_val = metrics_ [ key ] . get () if torch . is_tensor ( append_val ): if append_val . requires_grad : append_val = append_val . detach () . cpu () . item () history [ key ] . append ( append_val ) metrics_ [ key ] . reset () else : history [ \"loss\" ] . append ( epoch_avg_loss ) # weight saving if self . save_weights_every_n_epochs is not None : if self . epoch_num % self . save_weights_every_n_epochs == 0 : if not os . path . exists ( \"saved\" ): os . makedirs ( \"saved\" ) save_path = \"saved/epoch_\" + str ( self . epoch_num ) + \"-\" + self . experiment_name self . save_weights ( save_path ) self . on_epoch_end () self . model . eval () print ( 'Finished Training!' ) self . epoch_num = 0 self . on_run_end () return history","title":"Experiment"},{"location":"api/experiment/#torchplate.experiment--file-experimentpy","text":"Provides the main module of the package: Experiment.","title":"File: experiment.py"},{"location":"api/experiment/#torchplate.experiment.Experiment","text":"Bases: ABC Base experiment superclass. All other experiments should inherit from this class. Each sub-experiment must provide an implementation of the \"evaluate\" abstract method. A sub-experiment has full autonomy to override the basic components such as the training loop \"train\". Source code in torchplate/experiment.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 class Experiment ( ABC ): \"\"\" Base experiment superclass. All other experiments should inherit from this class. Each sub-experiment must provide an implementation of the \"evaluate\" abstract method. A sub-experiment has full autonomy to override the basic components such as the training loop \"train\". \"\"\" def __init__ ( self , model , optimizer , trainloader , save_weights_every_n_epochs = None , wandb_logger = None , verbose = True , experiment_name = misc . timestamp ()): \"\"\" Experiment superclass initializer. Each subclass must provide a model, optimizer, and trainloader at the very least. Arguments: ----------- - model: torch nn.module - optimizer: torch optimizer - trainloader: torch Dataloader to be used for training Optional: - save_weights_every_n_epochs: how often to save the model weight automatically. Default: None. Specify None if you don't want to save weights automatically. - wandb_logger (wandb.init object): pass in if you want to log to wandb. Default: None. - verbose (boolean): if true, print out metrics during training. Default: True. - experiment_name (str): name of the experiment for saving. Default: timestamp. \"\"\" self . model = model self . optimizer = optimizer self . trainloader = trainloader self . wandb_logger = wandb_logger self . verbose = verbose self . save_weights_every_n_epochs = save_weights_every_n_epochs assert type ( self . save_weights_every_n_epochs ) is int or self . save_weights_every_n_epochs is None , \"save_weights_every_n_epochs must be an integer or None\" self . epoch_num = 0 self . experiment_name = experiment_name assert type ( self . experiment_name ) is str , \"experiment name must be a string\" def train ( self , num_epochs , gradient_accumulate_every_n_batches = 1 , display_batch_loss = False ): \"\"\" Training loop. Can optionally specify how often to accumulate gradients. Default: 1. \"\"\" self . on_run_start () self . model . train () metrics_ = {} first_batch = True eval_returns_loss_only = False history = {} for epoch in range ( num_epochs ): self . epoch_num += 1 self . on_epoch_start () running_loss = 0.0 tqdm_loader = tqdm ( self . trainloader ) batch_idx = - 1 for batch in tqdm_loader : batch_idx += 1 if not display_batch_loss : tqdm_loader . set_description ( f \"Epoch { self . epoch_num } \" ) self . on_batch_start () evals = self . evaluate ( batch ) # registration of metrics if first_batch : if torch . is_tensor ( evals ): if evals . dim () == 0 : eval_returns_loss_only = True history [ \"loss\" ] = [] else : assert type ( evals ) is dict , \"if you aren't providing a scalar loss value in evaluate, you must return a dictionary.\" assert \"loss\" in evals . keys (), \"evaluate must return a 'loss' value\" # register metrics for key in evals : curr_metric = metrics . MeanMetric () metrics_ [ key ] = curr_metric history [ key ] = [] first_batch = False # get loss val if eval_returns_loss_only : loss = evals else : loss = evals [ \"loss\" ] if display_batch_loss : tqdm_loader . set_description ( f \"Epoch { self . epoch_num } | loss: { loss : .4f } \" ) loss . backward () # gradient accumulation if gradient_accumulate_every_n_batches > 1 : loss = loss / gradient_accumulate_every_n_batches if (( batch_idx + 1 ) % gradient_accumulate_every_n_batches == 0 ) or ( batch_idx + 1 == len ( self . trainloader )): self . optimizer . step () self . optimizer . zero_grad () else : self . optimizer . step () self . optimizer . zero_grad () # per-batch updates if not eval_returns_loss_only : # update metrics for key in metrics_ : metrics_ [ key ] . update ( evals [ key ]) running_loss += loss . item () self . on_batch_end () # per-epoch updates epoch_avg_loss = running_loss / len ( self . trainloader ) if self . wandb_logger is not None : if eval_returns_loss_only : self . wandb_logger . log ({ \"Training loss\" : epoch_avg_loss }) else : self . wandb_logger . log ( metrics_ ) if self . verbose : if eval_returns_loss_only : print ( \"Training Loss (epoch \" + str ( self . epoch_num ) + \"):\" , epoch_avg_loss ) else : for key in metrics_ : print ( f \"Training { key } (epoch { str ( self . epoch_num ) } ): { metrics_ [ key ] . get () } \" ) # reset metrics and update history if not eval_returns_loss_only : for key in metrics_ : append_val = metrics_ [ key ] . get () if torch . is_tensor ( append_val ): if append_val . requires_grad : append_val = append_val . detach () . cpu () . item () history [ key ] . append ( append_val ) metrics_ [ key ] . reset () else : history [ \"loss\" ] . append ( epoch_avg_loss ) # weight saving if self . save_weights_every_n_epochs is not None : if self . epoch_num % self . save_weights_every_n_epochs == 0 : if not os . path . exists ( \"saved\" ): os . makedirs ( \"saved\" ) save_path = \"saved/epoch_\" + str ( self . epoch_num ) + \"-\" + self . experiment_name self . save_weights ( save_path ) self . on_epoch_end () self . model . eval () print ( 'Finished Training!' ) self . epoch_num = 0 self . on_run_end () return history @abstractmethod def evaluate ( self , batch ): \"\"\" Abstract method which the user must provide. Implement the forward pass and return the loss value. Arguments: ----------- - batch: a batch from the train data loader (i.e., an (x, y) pair). To be used as input into the model. Returns: ----------- - A scalar loss value. \"\"\" pass def on_batch_start ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen before each batch iteration. \"\"\" pass def on_batch_end ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen after each batch iteration. \"\"\" pass def on_epoch_start ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen before each epoch iteration. \"\"\" pass def on_epoch_end ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen after each epoch iteration. \"\"\" pass def on_run_start ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen before each run. \"\"\" pass def on_run_end ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen after each run. \"\"\" pass def save_weights ( self , save_path = None ): \"\"\" Function to save model weights at 'save_path'. Arguments: - save_path: path to save the weights. If not given, defaults to current timestamp. \"\"\" if save_path is None : if not os . path . exists ( \"saved\" ): os . makedirs ( \"saved\" ) save_path = \"saved/\" + misc . timestamp () + \".pth\" torch . save ( self . model . state_dict (), save_path ) print ( \"Model weights saved at: \" + str ( save_path )) def load_weights ( self , weight_path ): \"\"\" Function to load model weights saved at 'weight_path'. Arguments: - weight_path: path pointing to the saved weights. \"\"\" self . model . load_state_dict ( torch . load ( weight_path )) print ( \"Weights loaded!\" )","title":"Experiment"},{"location":"api/experiment/#torchplate.experiment.Experiment.__init__","text":"Experiment superclass initializer. Each subclass must provide a model, optimizer, and trainloader at the very least. Arguments: model: torch nn.module optimizer: torch optimizer trainloader: torch Dataloader to be used for training Optional: save_weights_every_n_epochs: how often to save the model weight automatically. Default: None. Specify None if you don't want to save weights automatically. wandb_logger (wandb.init object): pass in if you want to log to wandb. Default: None. verbose (boolean): if true, print out metrics during training. Default: True. experiment_name (str): name of the experiment for saving. Default: timestamp. Source code in torchplate/experiment.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , model , optimizer , trainloader , save_weights_every_n_epochs = None , wandb_logger = None , verbose = True , experiment_name = misc . timestamp ()): \"\"\" Experiment superclass initializer. Each subclass must provide a model, optimizer, and trainloader at the very least. Arguments: ----------- - model: torch nn.module - optimizer: torch optimizer - trainloader: torch Dataloader to be used for training Optional: - save_weights_every_n_epochs: how often to save the model weight automatically. Default: None. Specify None if you don't want to save weights automatically. - wandb_logger (wandb.init object): pass in if you want to log to wandb. Default: None. - verbose (boolean): if true, print out metrics during training. Default: True. - experiment_name (str): name of the experiment for saving. Default: timestamp. \"\"\" self . model = model self . optimizer = optimizer self . trainloader = trainloader self . wandb_logger = wandb_logger self . verbose = verbose self . save_weights_every_n_epochs = save_weights_every_n_epochs assert type ( self . save_weights_every_n_epochs ) is int or self . save_weights_every_n_epochs is None , \"save_weights_every_n_epochs must be an integer or None\" self . epoch_num = 0 self . experiment_name = experiment_name assert type ( self . experiment_name ) is str , \"experiment name must be a string\"","title":"__init__()"},{"location":"api/experiment/#torchplate.experiment.Experiment.evaluate","text":"Abstract method which the user must provide. Implement the forward pass and return the loss value. batch: a batch from the train data loader (i.e., an (x, y) pair). To be used as input into the model. A scalar loss value. Source code in torchplate/experiment.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 @abstractmethod def evaluate ( self , batch ): \"\"\" Abstract method which the user must provide. Implement the forward pass and return the loss value. Arguments: ----------- - batch: a batch from the train data loader (i.e., an (x, y) pair). To be used as input into the model. Returns: ----------- - A scalar loss value. \"\"\" pass","title":"evaluate()"},{"location":"api/experiment/#torchplate.experiment.Experiment.load_weights","text":"Function to load model weights saved at 'weight_path'. weight_path: path pointing to the saved weights. Source code in torchplate/experiment.py 255 256 257 258 259 260 261 262 def load_weights ( self , weight_path ): \"\"\" Function to load model weights saved at 'weight_path'. Arguments: - weight_path: path pointing to the saved weights. \"\"\" self . model . load_state_dict ( torch . load ( weight_path )) print ( \"Weights loaded!\" )","title":"load_weights()"},{"location":"api/experiment/#torchplate.experiment.Experiment.on_batch_end","text":"Callback that can be overriden. Implement whatever you want to happen after each batch iteration. Source code in torchplate/experiment.py 201 202 203 204 205 206 def on_batch_end ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen after each batch iteration. \"\"\" pass","title":"on_batch_end()"},{"location":"api/experiment/#torchplate.experiment.Experiment.on_batch_start","text":"Callback that can be overriden. Implement whatever you want to happen before each batch iteration. Source code in torchplate/experiment.py 193 194 195 196 197 198 def on_batch_start ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen before each batch iteration. \"\"\" pass","title":"on_batch_start()"},{"location":"api/experiment/#torchplate.experiment.Experiment.on_epoch_end","text":"Callback that can be overriden. Implement whatever you want to happen after each epoch iteration. Source code in torchplate/experiment.py 217 218 219 220 221 222 def on_epoch_end ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen after each epoch iteration. \"\"\" pass","title":"on_epoch_end()"},{"location":"api/experiment/#torchplate.experiment.Experiment.on_epoch_start","text":"Callback that can be overriden. Implement whatever you want to happen before each epoch iteration. Source code in torchplate/experiment.py 209 210 211 212 213 214 def on_epoch_start ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen before each epoch iteration. \"\"\" pass","title":"on_epoch_start()"},{"location":"api/experiment/#torchplate.experiment.Experiment.on_run_end","text":"Callback that can be overriden. Implement whatever you want to happen after each run. Source code in torchplate/experiment.py 233 234 235 236 237 238 def on_run_end ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen after each run. \"\"\" pass","title":"on_run_end()"},{"location":"api/experiment/#torchplate.experiment.Experiment.on_run_start","text":"Callback that can be overriden. Implement whatever you want to happen before each run. Source code in torchplate/experiment.py 225 226 227 228 229 230 def on_run_start ( self ): \"\"\" Callback that can be overriden. Implement whatever you want to happen before each run. \"\"\" pass","title":"on_run_start()"},{"location":"api/experiment/#torchplate.experiment.Experiment.save_weights","text":"Function to save model weights at 'save_path'. save_path: path to save the weights. If not given, defaults to current timestamp. Source code in torchplate/experiment.py 241 242 243 244 245 246 247 248 249 250 251 252 def save_weights ( self , save_path = None ): \"\"\" Function to save model weights at 'save_path'. Arguments: - save_path: path to save the weights. If not given, defaults to current timestamp. \"\"\" if save_path is None : if not os . path . exists ( \"saved\" ): os . makedirs ( \"saved\" ) save_path = \"saved/\" + misc . timestamp () + \".pth\" torch . save ( self . model . state_dict (), save_path ) print ( \"Model weights saved at: \" + str ( save_path ))","title":"save_weights()"},{"location":"api/experiment/#torchplate.experiment.Experiment.train","text":"Training loop. Can optionally specify how often to accumulate gradients. Default: 1. Source code in torchplate/experiment.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def train ( self , num_epochs , gradient_accumulate_every_n_batches = 1 , display_batch_loss = False ): \"\"\" Training loop. Can optionally specify how often to accumulate gradients. Default: 1. \"\"\" self . on_run_start () self . model . train () metrics_ = {} first_batch = True eval_returns_loss_only = False history = {} for epoch in range ( num_epochs ): self . epoch_num += 1 self . on_epoch_start () running_loss = 0.0 tqdm_loader = tqdm ( self . trainloader ) batch_idx = - 1 for batch in tqdm_loader : batch_idx += 1 if not display_batch_loss : tqdm_loader . set_description ( f \"Epoch { self . epoch_num } \" ) self . on_batch_start () evals = self . evaluate ( batch ) # registration of metrics if first_batch : if torch . is_tensor ( evals ): if evals . dim () == 0 : eval_returns_loss_only = True history [ \"loss\" ] = [] else : assert type ( evals ) is dict , \"if you aren't providing a scalar loss value in evaluate, you must return a dictionary.\" assert \"loss\" in evals . keys (), \"evaluate must return a 'loss' value\" # register metrics for key in evals : curr_metric = metrics . MeanMetric () metrics_ [ key ] = curr_metric history [ key ] = [] first_batch = False # get loss val if eval_returns_loss_only : loss = evals else : loss = evals [ \"loss\" ] if display_batch_loss : tqdm_loader . set_description ( f \"Epoch { self . epoch_num } | loss: { loss : .4f } \" ) loss . backward () # gradient accumulation if gradient_accumulate_every_n_batches > 1 : loss = loss / gradient_accumulate_every_n_batches if (( batch_idx + 1 ) % gradient_accumulate_every_n_batches == 0 ) or ( batch_idx + 1 == len ( self . trainloader )): self . optimizer . step () self . optimizer . zero_grad () else : self . optimizer . step () self . optimizer . zero_grad () # per-batch updates if not eval_returns_loss_only : # update metrics for key in metrics_ : metrics_ [ key ] . update ( evals [ key ]) running_loss += loss . item () self . on_batch_end () # per-epoch updates epoch_avg_loss = running_loss / len ( self . trainloader ) if self . wandb_logger is not None : if eval_returns_loss_only : self . wandb_logger . log ({ \"Training loss\" : epoch_avg_loss }) else : self . wandb_logger . log ( metrics_ ) if self . verbose : if eval_returns_loss_only : print ( \"Training Loss (epoch \" + str ( self . epoch_num ) + \"):\" , epoch_avg_loss ) else : for key in metrics_ : print ( f \"Training { key } (epoch { str ( self . epoch_num ) } ): { metrics_ [ key ] . get () } \" ) # reset metrics and update history if not eval_returns_loss_only : for key in metrics_ : append_val = metrics_ [ key ] . get () if torch . is_tensor ( append_val ): if append_val . requires_grad : append_val = append_val . detach () . cpu () . item () history [ key ] . append ( append_val ) metrics_ [ key ] . reset () else : history [ \"loss\" ] . append ( epoch_avg_loss ) # weight saving if self . save_weights_every_n_epochs is not None : if self . epoch_num % self . save_weights_every_n_epochs == 0 : if not os . path . exists ( \"saved\" ): os . makedirs ( \"saved\" ) save_path = \"saved/epoch_\" + str ( self . epoch_num ) + \"-\" + self . experiment_name self . save_weights ( save_path ) self . on_epoch_end () self . model . eval () print ( 'Finished Training!' ) self . epoch_num = 0 self . on_run_end () return history","title":"train()"},{"location":"api/metrics/","text":"File: metrics.py Metric classes for evaluating model performance. Accuracy Bases: MeanMetric Subclass of MeanMetric which defines a standard accuracy update function. Source code in torchplate/metrics.py 34 35 36 37 38 39 40 41 class Accuracy ( MeanMetric ): \"\"\" Subclass of MeanMetric which defines a standard accuracy update function. \"\"\" def update ( self , logits , labels ): return calculate_accuracy ( logits , labels ) MeanMetric Scalar metric designed to use on a per-epoch basis and updated on per-batch basis. For getting average across the epoch. Source code in torchplate/metrics.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class MeanMetric : \"\"\" Scalar metric designed to use on a per-epoch basis and updated on per-batch basis. For getting average across the epoch. \"\"\" def __init__ ( self ): self . vals = [] def update ( self , new_val ): self . vals . append ( new_val ) def reset ( self ): self . vals = [] def get ( self ): mean_value = sum ( self . vals ) / len ( self . vals ) return mean_value MeanMetricCustom Bases: ABC , MeanMetric Abstract scalar metric. Must provide calculation given logits and y. Source code in torchplate/metrics.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class MeanMetricCustom ( ABC , MeanMetric ): \"\"\" Abstract scalar metric. Must provide calculation given logits and y. \"\"\" def __init__ ( self ): self . vals = [] @abstractmethod def calculate ( self , logits , y ): # returns a value pass def update ( self , logits , y ): self . vals . append ( self . calculate ( logits , y ))","title":"Metrics"},{"location":"api/metrics/#torchplate.metrics--file-metricspy","text":"Metric classes for evaluating model performance.","title":"File: metrics.py"},{"location":"api/metrics/#torchplate.metrics.Accuracy","text":"Bases: MeanMetric Subclass of MeanMetric which defines a standard accuracy update function. Source code in torchplate/metrics.py 34 35 36 37 38 39 40 41 class Accuracy ( MeanMetric ): \"\"\" Subclass of MeanMetric which defines a standard accuracy update function. \"\"\" def update ( self , logits , labels ): return calculate_accuracy ( logits , labels )","title":"Accuracy"},{"location":"api/metrics/#torchplate.metrics.MeanMetric","text":"Scalar metric designed to use on a per-epoch basis and updated on per-batch basis. For getting average across the epoch. Source code in torchplate/metrics.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class MeanMetric : \"\"\" Scalar metric designed to use on a per-epoch basis and updated on per-batch basis. For getting average across the epoch. \"\"\" def __init__ ( self ): self . vals = [] def update ( self , new_val ): self . vals . append ( new_val ) def reset ( self ): self . vals = [] def get ( self ): mean_value = sum ( self . vals ) / len ( self . vals ) return mean_value","title":"MeanMetric"},{"location":"api/metrics/#torchplate.metrics.MeanMetricCustom","text":"Bases: ABC , MeanMetric Abstract scalar metric. Must provide calculation given logits and y. Source code in torchplate/metrics.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class MeanMetricCustom ( ABC , MeanMetric ): \"\"\" Abstract scalar metric. Must provide calculation given logits and y. \"\"\" def __init__ ( self ): self . vals = [] @abstractmethod def calculate ( self , logits , y ): # returns a value pass def update ( self , logits , y ): self . vals . append ( self . calculate ( logits , y ))","title":"MeanMetricCustom"},{"location":"api/utils/","text":"File: utils.py We also provide some experiment workflow utilities which one can use and import from this module. BaseModelInterface Wrapper class which provides a model interface for torch.nn models. Mainly, this class provides the forward pass pipeline function, 'predict' which sends an input through this pipeline preprocess --> model --> postprocess. Users must provide a torch.nn model and can optionally specify preprocess and postprocess functions. Source code in torchplate/utils.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class BaseModelInterface : \"\"\" Wrapper class which provides a model interface for torch.nn models. Mainly, this class provides the forward pass pipeline function, 'predict' which sends an input through this pipeline: preprocess --> model --> postprocess. Users must provide a torch.nn model and can optionally specify preprocess and postprocess functions. \"\"\" def __init__ ( self , model ): \"\"\" Provide torch.nn module. \"\"\" self . model def preprocess ( self , inputs ): return inputs def postprocess ( self , inputs ): return inputs def forward_pipeline ( self , inputs ): preprocessed_inputs = self . preprocess ( inputs ) logits = self . model ( preprocessed_inputs ) processed_output = self . postprocess ( logits ) return processed_output def save_weights ( self , save_path = None ): if save_path is None : if not os . path . exists ( \"saved\" ): os . makedirs ( \"saved\" ) save_path = \"saved/\" + misc . timestamp () + \".pth\" torch . save ( self . model . state_dict (), save_path ) print ( \"Model weights saved at: \" + str ( save_path )) def load_weights ( self , weight_path ): self . model . load_state_dict ( torch . load ( weight_path )) print ( \"weights loaded!\" ) __init__ ( model ) Provide torch.nn module. Source code in torchplate/utils.py 28 29 30 31 32 def __init__ ( self , model ): \"\"\" Provide torch.nn module. \"\"\" self . model XYDataset Bases: Dataset PyTorch Dataset class for datasets of the form [(x,y), ..., (x,y)]. Source code in torchplate/utils.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class XYDataset ( Dataset ): \"\"\" PyTorch Dataset class for datasets of the form [(x,y), ..., (x,y)]. \"\"\" def __init__ ( self , data_set ): \"\"\" Arguments: ----------- - distribution (sequence): sequence of the form [(x,y), ..., (x,y)] representing the dataset. \"\"\" self . data_distribution = data_set def __getitem__ ( self , index ): sample = self . data_distribution [ index ][ 0 ] label = self . data_distribution [ index ][ 1 ] sample = torch . tensor ( sample , dtype = torch . float ) label = torch . tensor ( label ) return ( sample , label ) def __len__ ( self ): return len ( self . data_distribution ) __init__ ( data_set ) distribution (sequence): sequence of the form [(x,y), ..., (x,y)] representing the dataset. Source code in torchplate/utils.py 64 65 66 67 68 69 70 71 def __init__ ( self , data_set ): \"\"\" Arguments: ----------- - distribution (sequence): sequence of the form [(x,y), ..., (x,y)] representing the dataset. \"\"\" self . data_distribution = data_set get_loaders ( torch_sets ) Given a sequence of torch.utils.data.Dataset objects, this function wraps them all in torch.utils.data.Dataloader objects and returns a sequence in the same order. Note that this function doesn't support custom arguments to the torch.utils.data.DataLoader call. If one desires to use custom arguments (e.g., batch_size), they should call torch.utils.data.DataLoader themselves. torch_sets (sequence): a sequence consisting of a torch.utils.data.Dataset objects. loaders (sequence): the datasets wrapped in a torch.utils.data.Dataloader objects (returned in the same order.) Source code in torchplate/utils.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def get_loaders ( torch_sets ): \"\"\" Given a sequence of torch.utils.data.Dataset objects, this function wraps them all in torch.utils.data.Dataloader objects and returns a sequence in the same order. Note that this function doesn't support custom arguments to the torch.utils.data.DataLoader call. If one desires to use custom arguments (e.g., batch_size), they should call torch.utils.data.DataLoader themselves. Arguments: ----------- - torch_sets (sequence): a sequence consisting of a torch.utils.data.Dataset objects. Returns: ----------- - loaders (sequence): the datasets wrapped in a torch.utils.data.Dataloader objects (returned in the same order.) \"\"\" loaders = [] for torch_set in torch_sets : current_set = torch . utils . data . DataLoader ( torch_set ) loaders . append ( current_set ) return loaders get_xy_dataset ( distribution ) Given a dataset of the form [(x,y), ..., (x,y)], returns a PyTorch Dataset object. distribution (sequence): sequence of the form [(x,y), ..., (x,y)] representing the dataset. a torch.utils.data.Dataset object Source code in torchplate/utils.py 84 85 86 87 88 89 90 91 92 93 94 95 96 def get_xy_dataset ( distribution ): \"\"\" Given a dataset of the form [(x,y), ..., (x,y)], returns a PyTorch Dataset object. Arguments: ----------- - distribution (sequence): sequence of the form [(x,y), ..., (x,y)] representing the dataset. Returns: ----------- - a torch.utils.data.Dataset object \"\"\" return XYDataset ( distribution ) get_xy_loaders ( distribution ) end-to-end function which returns train and test loaders given a sequence of the form [(x,y), ..., (x,y)]. If more customization is needed, please call the other utility functions individually. distribution (sequence): dataset of the form [(x,y), ..., (x,y)]. loaders (sequence): the datasets wrapped in a torch.utils.data.Dataloader objects (returned in the same order). Source code in torchplate/utils.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def get_xy_loaders ( distribution ): \"\"\" end-to-end function which returns train and test loaders given a sequence of the form [(x,y), ..., (x,y)]. If more customization is needed, please call the other utility functions individually. Arguments: ----------- - distribution (sequence): dataset of the form [(x,y), ..., (x,y)]. Returns: ----------- - loaders (sequence): the datasets wrapped in a torch.utils.data.Dataloader objects (returned in the same order). \"\"\" torch_set = get_xy_dataset ( distribution ) torch_sets = split_dataset ( torch_set ) loaders = get_loaders ( torch_sets ) trainloader = loaders [ 0 ] testloader = loaders [ 1 ] return trainloader , testloader split_dataset ( torch_set , ratio = 0.9 ) Given a torch.utils.data.Dataset object, this function splits it into train and test a torch.utils.data.Dataset objects. The split is random is the size is based on the input ratio. torch_set: a torch.utils.data.Dataset object containing the entire dataset ratio: train/test ratio split. Default is 0.9. Tuple consisting of: - trainset: a torch.utils.data.Dataset object to be used for training - testset: a torch.utils.data.Dataset object to be used for testing Source code in torchplate/utils.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def split_dataset ( torch_set , ratio = 0.9 ): \"\"\" Given a torch.utils.data.Dataset object, this function splits it into train and test a torch.utils.data.Dataset objects. The split is random is the size is based on the input ratio. Arguments: ----------- - torch_set: a torch.utils.data.Dataset object containing the entire dataset - ratio: train/test ratio split. Default is 0.9. Returns: ----------- Tuple consisting of: - trainset: a torch.utils.data.Dataset object to be used for training - testset: a torch.utils.data.Dataset object to be used for testing \"\"\" train_size = int ( ratio * len ( torch_set )) test_size = len ( torch_set ) - train_size train_dataset , test_dataset = torch . utils . data . random_split ( torch_set , [ train_size , test_size ]) return train_dataset , test_dataset","title":"Utils"},{"location":"api/utils/#torchplate.utils--file-utilspy","text":"We also provide some experiment workflow utilities which one can use and import from this module.","title":"File: utils.py"},{"location":"api/utils/#torchplate.utils.BaseModelInterface","text":"Wrapper class which provides a model interface for torch.nn models. Mainly, this class provides the forward pass pipeline function, 'predict' which sends an input through this pipeline preprocess --> model --> postprocess. Users must provide a torch.nn model and can optionally specify preprocess and postprocess functions. Source code in torchplate/utils.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class BaseModelInterface : \"\"\" Wrapper class which provides a model interface for torch.nn models. Mainly, this class provides the forward pass pipeline function, 'predict' which sends an input through this pipeline: preprocess --> model --> postprocess. Users must provide a torch.nn model and can optionally specify preprocess and postprocess functions. \"\"\" def __init__ ( self , model ): \"\"\" Provide torch.nn module. \"\"\" self . model def preprocess ( self , inputs ): return inputs def postprocess ( self , inputs ): return inputs def forward_pipeline ( self , inputs ): preprocessed_inputs = self . preprocess ( inputs ) logits = self . model ( preprocessed_inputs ) processed_output = self . postprocess ( logits ) return processed_output def save_weights ( self , save_path = None ): if save_path is None : if not os . path . exists ( \"saved\" ): os . makedirs ( \"saved\" ) save_path = \"saved/\" + misc . timestamp () + \".pth\" torch . save ( self . model . state_dict (), save_path ) print ( \"Model weights saved at: \" + str ( save_path )) def load_weights ( self , weight_path ): self . model . load_state_dict ( torch . load ( weight_path )) print ( \"weights loaded!\" )","title":"BaseModelInterface"},{"location":"api/utils/#torchplate.utils.BaseModelInterface.__init__","text":"Provide torch.nn module. Source code in torchplate/utils.py 28 29 30 31 32 def __init__ ( self , model ): \"\"\" Provide torch.nn module. \"\"\" self . model","title":"__init__()"},{"location":"api/utils/#torchplate.utils.XYDataset","text":"Bases: Dataset PyTorch Dataset class for datasets of the form [(x,y), ..., (x,y)]. Source code in torchplate/utils.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class XYDataset ( Dataset ): \"\"\" PyTorch Dataset class for datasets of the form [(x,y), ..., (x,y)]. \"\"\" def __init__ ( self , data_set ): \"\"\" Arguments: ----------- - distribution (sequence): sequence of the form [(x,y), ..., (x,y)] representing the dataset. \"\"\" self . data_distribution = data_set def __getitem__ ( self , index ): sample = self . data_distribution [ index ][ 0 ] label = self . data_distribution [ index ][ 1 ] sample = torch . tensor ( sample , dtype = torch . float ) label = torch . tensor ( label ) return ( sample , label ) def __len__ ( self ): return len ( self . data_distribution )","title":"XYDataset"},{"location":"api/utils/#torchplate.utils.XYDataset.__init__","text":"distribution (sequence): sequence of the form [(x,y), ..., (x,y)] representing the dataset. Source code in torchplate/utils.py 64 65 66 67 68 69 70 71 def __init__ ( self , data_set ): \"\"\" Arguments: ----------- - distribution (sequence): sequence of the form [(x,y), ..., (x,y)] representing the dataset. \"\"\" self . data_distribution = data_set","title":"__init__()"},{"location":"api/utils/#torchplate.utils.get_loaders","text":"Given a sequence of torch.utils.data.Dataset objects, this function wraps them all in torch.utils.data.Dataloader objects and returns a sequence in the same order. Note that this function doesn't support custom arguments to the torch.utils.data.DataLoader call. If one desires to use custom arguments (e.g., batch_size), they should call torch.utils.data.DataLoader themselves. torch_sets (sequence): a sequence consisting of a torch.utils.data.Dataset objects. loaders (sequence): the datasets wrapped in a torch.utils.data.Dataloader objects (returned in the same order.) Source code in torchplate/utils.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def get_loaders ( torch_sets ): \"\"\" Given a sequence of torch.utils.data.Dataset objects, this function wraps them all in torch.utils.data.Dataloader objects and returns a sequence in the same order. Note that this function doesn't support custom arguments to the torch.utils.data.DataLoader call. If one desires to use custom arguments (e.g., batch_size), they should call torch.utils.data.DataLoader themselves. Arguments: ----------- - torch_sets (sequence): a sequence consisting of a torch.utils.data.Dataset objects. Returns: ----------- - loaders (sequence): the datasets wrapped in a torch.utils.data.Dataloader objects (returned in the same order.) \"\"\" loaders = [] for torch_set in torch_sets : current_set = torch . utils . data . DataLoader ( torch_set ) loaders . append ( current_set ) return loaders","title":"get_loaders()"},{"location":"api/utils/#torchplate.utils.get_xy_dataset","text":"Given a dataset of the form [(x,y), ..., (x,y)], returns a PyTorch Dataset object. distribution (sequence): sequence of the form [(x,y), ..., (x,y)] representing the dataset. a torch.utils.data.Dataset object Source code in torchplate/utils.py 84 85 86 87 88 89 90 91 92 93 94 95 96 def get_xy_dataset ( distribution ): \"\"\" Given a dataset of the form [(x,y), ..., (x,y)], returns a PyTorch Dataset object. Arguments: ----------- - distribution (sequence): sequence of the form [(x,y), ..., (x,y)] representing the dataset. Returns: ----------- - a torch.utils.data.Dataset object \"\"\" return XYDataset ( distribution )","title":"get_xy_dataset()"},{"location":"api/utils/#torchplate.utils.get_xy_loaders","text":"end-to-end function which returns train and test loaders given a sequence of the form [(x,y), ..., (x,y)]. If more customization is needed, please call the other utility functions individually. distribution (sequence): dataset of the form [(x,y), ..., (x,y)]. loaders (sequence): the datasets wrapped in a torch.utils.data.Dataloader objects (returned in the same order). Source code in torchplate/utils.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def get_xy_loaders ( distribution ): \"\"\" end-to-end function which returns train and test loaders given a sequence of the form [(x,y), ..., (x,y)]. If more customization is needed, please call the other utility functions individually. Arguments: ----------- - distribution (sequence): dataset of the form [(x,y), ..., (x,y)]. Returns: ----------- - loaders (sequence): the datasets wrapped in a torch.utils.data.Dataloader objects (returned in the same order). \"\"\" torch_set = get_xy_dataset ( distribution ) torch_sets = split_dataset ( torch_set ) loaders = get_loaders ( torch_sets ) trainloader = loaders [ 0 ] testloader = loaders [ 1 ] return trainloader , testloader","title":"get_xy_loaders()"},{"location":"api/utils/#torchplate.utils.split_dataset","text":"Given a torch.utils.data.Dataset object, this function splits it into train and test a torch.utils.data.Dataset objects. The split is random is the size is based on the input ratio. torch_set: a torch.utils.data.Dataset object containing the entire dataset ratio: train/test ratio split. Default is 0.9. Tuple consisting of: - trainset: a torch.utils.data.Dataset object to be used for training - testset: a torch.utils.data.Dataset object to be used for testing Source code in torchplate/utils.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def split_dataset ( torch_set , ratio = 0.9 ): \"\"\" Given a torch.utils.data.Dataset object, this function splits it into train and test a torch.utils.data.Dataset objects. The split is random is the size is based on the input ratio. Arguments: ----------- - torch_set: a torch.utils.data.Dataset object containing the entire dataset - ratio: train/test ratio split. Default is 0.9. Returns: ----------- Tuple consisting of: - trainset: a torch.utils.data.Dataset object to be used for training - testset: a torch.utils.data.Dataset object to be used for testing \"\"\" train_size = int ( ratio * len ( torch_set )) test_size = len ( torch_set ) - train_size train_dataset , test_dataset = torch . utils . data . random_split ( torch_set , [ train_size , test_size ]) return train_dataset , test_dataset","title":"split_dataset()"},{"location":"examples/segmentation/","text":"Segmentation This page showcases an example using torchplate to train a visual segmentation model. The program is divided into several .py files. configs.py \"\"\" File: configs.py ---------------------- Specifies config parameters. \"\"\" import datasets import models import experiments import torchplate import rsbox from rsbox import ml, misc import torch.optim as optim import segmentation_models_pytorch as smp class BaseConfig: experiment = experiments.BaseExp dataset_dist = misc.load_dataset(\"https://stanford.edu/~rsikand/assets/datasets/mini_cell_segmentation.pkl\") trainloader, testloader = torchplate.utils.get_xy_loaders(dataset_dist) model_class = models.SmpUnet( encoder_name='resnet34', encoder_weights='imagenet', classes=1, in_channels=3, activation='sigmoid' ) loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True) optimizer = optim.Adam(model_class.model.parameters(), lr=0.001) experiments.py \"\"\" File: experiments.py ------------------ This file holds the experiments which are subclasses of torchplate.experiment.Experiment. \"\"\" import numpy as np import torchplate from torchplate import ( experiment, utils ) import pdb import wandb import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F import models import segmentation_models_pytorch as smp class BaseExp(experiment.Experiment): def __init__(self, config): self.cfg = config self.model_class = self.cfg.model_class self.model = self.model_class.model self.trainloader = self.cfg.trainloader self.testloader = self.cfg.testloader self.criterion = self.cfg.loss_fn self.optimizer = self.cfg.optimizer super().__init__( model = self.model, optimizer = self.optimizer, trainloader = self.trainloader, wandb_logger = None, verbose = True ) # provide this abstract method to calculate loss def evaluate(self, batch): x, y = batch logits = self.model_class.forward_pipeline(x) # y_un_one_hot = torch.argmax(y, dim=1) y_un_one_hot = y[:, 1, :, :] loss_val = self.criterion(logits, y_un_one_hot) return loss_val @staticmethod def compute_iou(logits, y): # first compute statistics for true positives, false positives, false negative and # true negative \"pixels\" tp, fp, fn, tn = smp.metrics.get_stats(logits, y.long(), mode='binary', threshold=0.5) iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\") return iou_score def test(self): # test the model on the test set iou_scores = [] with torch.no_grad(): for batch in self.testloader: x, y = batch logits = self.model_class.forward_pipeline(x) y_un_one_hot = y[:, 1:, :, :] iou_score = self.compute_iou(logits, y_un_one_hot) iou_scores.append(iou_score) iou_score_avg = np.mean(np.array(iou_scores)) print(f\"Average IoU score (testset): {iou_score_avg}\") models.py \"\"\" File: models.py ------------------ This file holds the torch.nn modules. \"\"\" import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F import torchplate from torchplate import ( experiment, utils ) import segmentation_models_pytorch as smp class SmpUnet(utils.BaseModelInterface): # note: each child should provide a dict containing the relevant model params if needed def __init__(self, encoder_name='resnet34', encoder_weights='imagenet', classes=1, in_channels=1, activation='sigmoid'): # model self.model = smp.Unet( encoder_name=encoder_name, encoder_weights=encoder_weights, classes=classes, in_channels=in_channels, activation=activation ) # preprocessing self.preprocessing_fn = smp.encoders.get_preprocessing_fn( encoder_name=encoder_name, pretrained=encoder_weights ) super().__init__( model = self.model ) def preprocess(self, inputs): # input should be of shape (n, c, h, w) if self.preprocessing_fn is not None: # preprocess input inputs = torch.movedim(inputs, 1, -1) # (n, c, h, w) --> (n, h, w, c) inputs = self.preprocessing_fn(inputs) inputs = torch.movedim(inputs, -1, 1) # (n, h, w, c) --> (n, c, h, w) inputs = inputs.to(torch.float) return inputs runner.py \"\"\" File: runner.py ------------------ Runner script to train the model. This is the script that calls the other modules. Execute this one to execute the program! \"\"\" import configs import argparse import warnings import pdb import experiments def main(args): if args.config is None: config_class = 'BaseConfig' else: config_class = args.config cfg = getattr(configs, config_class) exp = cfg.experiment( config=cfg ) # train the model exp.test() exp.train(num_epochs=15) exp.test() if __name__ == '__main__': # configure args parser = argparse.ArgumentParser(description=\"specify cli arguments.\", allow_abbrev=True) parser.add_argument(\"-config\", type=str, help='specify config.py class to use.') args = parser.parse_args() main(args) Output Run: $ python3 run.py -c BaseConfig Output: Average IoU score (testset): 0.03814944624900818 Epoch 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:06<00:00, 1.79it/s] Training Loss (epoch 1): 0.6883486130020835 Epoch 2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.87it/s] Training Loss (epoch 2): 0.6678070100871 Epoch 3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.87it/s] Training Loss (epoch 3): 0.6593567999926481 Epoch 4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.87it/s] Training Loss (epoch 4): 0.6544880541888151 Epoch 5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.87it/s] Training Loss (epoch 5): 0.6512440117922697 Epoch 6: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.87it/s] Training Loss (epoch 6): 0.6492173671722412 Epoch 7: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.87it/s] Training Loss (epoch 7): 0.6479388150301847 Epoch 8: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.87it/s] Training Loss (epoch 8): 0.6467715881087563 Epoch 9: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.88it/s] Training Loss (epoch 9): 0.6455191428011114 Epoch 10: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.85it/s] Training Loss (epoch 10): 0.6443771774118597 Epoch 11: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.84it/s] Training Loss (epoch 11): 0.6441563692959872 Epoch 12: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.86it/s] Training Loss (epoch 12): 0.6434326388619163 Epoch 13: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.86it/s] Training Loss (epoch 13): 0.6425378214229237 Epoch 14: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:06<00:00, 1.72it/s] Training Loss (epoch 14): 0.6431863037022677 Epoch 15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.85it/s] Training Loss (epoch 15): 0.6430070671168241 Finished Training! Average IoU score (testset): 0.10699300467967987","title":"Segmentation"},{"location":"examples/segmentation/#segmentation","text":"This page showcases an example using torchplate to train a visual segmentation model. The program is divided into several .py files.","title":"Segmentation"},{"location":"examples/segmentation/#configspy","text":"\"\"\" File: configs.py ---------------------- Specifies config parameters. \"\"\" import datasets import models import experiments import torchplate import rsbox from rsbox import ml, misc import torch.optim as optim import segmentation_models_pytorch as smp class BaseConfig: experiment = experiments.BaseExp dataset_dist = misc.load_dataset(\"https://stanford.edu/~rsikand/assets/datasets/mini_cell_segmentation.pkl\") trainloader, testloader = torchplate.utils.get_xy_loaders(dataset_dist) model_class = models.SmpUnet( encoder_name='resnet34', encoder_weights='imagenet', classes=1, in_channels=3, activation='sigmoid' ) loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True) optimizer = optim.Adam(model_class.model.parameters(), lr=0.001)","title":"configs.py"},{"location":"examples/segmentation/#experimentspy","text":"\"\"\" File: experiments.py ------------------ This file holds the experiments which are subclasses of torchplate.experiment.Experiment. \"\"\" import numpy as np import torchplate from torchplate import ( experiment, utils ) import pdb import wandb import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F import models import segmentation_models_pytorch as smp class BaseExp(experiment.Experiment): def __init__(self, config): self.cfg = config self.model_class = self.cfg.model_class self.model = self.model_class.model self.trainloader = self.cfg.trainloader self.testloader = self.cfg.testloader self.criterion = self.cfg.loss_fn self.optimizer = self.cfg.optimizer super().__init__( model = self.model, optimizer = self.optimizer, trainloader = self.trainloader, wandb_logger = None, verbose = True ) # provide this abstract method to calculate loss def evaluate(self, batch): x, y = batch logits = self.model_class.forward_pipeline(x) # y_un_one_hot = torch.argmax(y, dim=1) y_un_one_hot = y[:, 1, :, :] loss_val = self.criterion(logits, y_un_one_hot) return loss_val @staticmethod def compute_iou(logits, y): # first compute statistics for true positives, false positives, false negative and # true negative \"pixels\" tp, fp, fn, tn = smp.metrics.get_stats(logits, y.long(), mode='binary', threshold=0.5) iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\") return iou_score def test(self): # test the model on the test set iou_scores = [] with torch.no_grad(): for batch in self.testloader: x, y = batch logits = self.model_class.forward_pipeline(x) y_un_one_hot = y[:, 1:, :, :] iou_score = self.compute_iou(logits, y_un_one_hot) iou_scores.append(iou_score) iou_score_avg = np.mean(np.array(iou_scores)) print(f\"Average IoU score (testset): {iou_score_avg}\")","title":"experiments.py"},{"location":"examples/segmentation/#modelspy","text":"\"\"\" File: models.py ------------------ This file holds the torch.nn modules. \"\"\" import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F import torchplate from torchplate import ( experiment, utils ) import segmentation_models_pytorch as smp class SmpUnet(utils.BaseModelInterface): # note: each child should provide a dict containing the relevant model params if needed def __init__(self, encoder_name='resnet34', encoder_weights='imagenet', classes=1, in_channels=1, activation='sigmoid'): # model self.model = smp.Unet( encoder_name=encoder_name, encoder_weights=encoder_weights, classes=classes, in_channels=in_channels, activation=activation ) # preprocessing self.preprocessing_fn = smp.encoders.get_preprocessing_fn( encoder_name=encoder_name, pretrained=encoder_weights ) super().__init__( model = self.model ) def preprocess(self, inputs): # input should be of shape (n, c, h, w) if self.preprocessing_fn is not None: # preprocess input inputs = torch.movedim(inputs, 1, -1) # (n, c, h, w) --> (n, h, w, c) inputs = self.preprocessing_fn(inputs) inputs = torch.movedim(inputs, -1, 1) # (n, h, w, c) --> (n, c, h, w) inputs = inputs.to(torch.float) return inputs","title":"models.py"},{"location":"examples/segmentation/#runnerpy","text":"\"\"\" File: runner.py ------------------ Runner script to train the model. This is the script that calls the other modules. Execute this one to execute the program! \"\"\" import configs import argparse import warnings import pdb import experiments def main(args): if args.config is None: config_class = 'BaseConfig' else: config_class = args.config cfg = getattr(configs, config_class) exp = cfg.experiment( config=cfg ) # train the model exp.test() exp.train(num_epochs=15) exp.test() if __name__ == '__main__': # configure args parser = argparse.ArgumentParser(description=\"specify cli arguments.\", allow_abbrev=True) parser.add_argument(\"-config\", type=str, help='specify config.py class to use.') args = parser.parse_args() main(args)","title":"runner.py"},{"location":"examples/segmentation/#output","text":"Run: $ python3 run.py -c BaseConfig Output: Average IoU score (testset): 0.03814944624900818 Epoch 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:06<00:00, 1.79it/s] Training Loss (epoch 1): 0.6883486130020835 Epoch 2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.87it/s] Training Loss (epoch 2): 0.6678070100871 Epoch 3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.87it/s] Training Loss (epoch 3): 0.6593567999926481 Epoch 4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.87it/s] Training Loss (epoch 4): 0.6544880541888151 Epoch 5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.87it/s] Training Loss (epoch 5): 0.6512440117922697 Epoch 6: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.87it/s] Training Loss (epoch 6): 0.6492173671722412 Epoch 7: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.87it/s] Training Loss (epoch 7): 0.6479388150301847 Epoch 8: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.87it/s] Training Loss (epoch 8): 0.6467715881087563 Epoch 9: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.88it/s] Training Loss (epoch 9): 0.6455191428011114 Epoch 10: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.85it/s] Training Loss (epoch 10): 0.6443771774118597 Epoch 11: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.84it/s] Training Loss (epoch 11): 0.6441563692959872 Epoch 12: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.86it/s] Training Loss (epoch 12): 0.6434326388619163 Epoch 13: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.86it/s] Training Loss (epoch 13): 0.6425378214229237 Epoch 14: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:06<00:00, 1.72it/s] Training Loss (epoch 14): 0.6431863037022677 Epoch 15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:05<00:00, 1.85it/s] Training Loss (epoch 15): 0.6430070671168241 Finished Training! Average IoU score (testset): 0.10699300467967987","title":"Output"}]}